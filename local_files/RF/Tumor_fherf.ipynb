{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iDASH Tumor classification# iDASH2020 tumor classification 데이터 분석 모델\n",
    "\n",
    "본 문서에서는 'tumor classficiation' 모델 구동을 설명한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 개요\n",
    "Tumor 데이이터 분석은 세 단계로 나누어진다. \n",
    "1. tumor 데이터 전처리\n",
    "2. Scikit-learn 모델 훈련\n",
    "3. FHE neural RF 모델 훈련 및 활용\n",
    "\n",
    "## Tumor data\n",
    "Tumor 데이터는 상당히 비균질한 특성을 가지므로 다양한 전처리 기술이 필요하다.  \n",
    "[Hong et al. 2021](https://doi.org/10.21203/rs.3.rs-584746/v1)연구에서는 모델의 정확도를 향상시키기 위해 유사한 gene을 그룹화하고, 영향이 적은 gene mutation을 걸러내는 등 도메인 지식에 기반한 다양한 전처리 작업을 수행하였다.  \n",
    "이 과제는 FHE 활용 RF 모델을 만드는 것에 초점을 두므로 고도의 전처리는 포함하지 않는다. FHE 모델은 Scikit-learn으로 훈련된 최종 모델을 참조하므로 추후에 전처리 단계를 고도화하여 모델의 정확도를 향상시킬 수 있다. \n",
    "\n",
    "## 전처리\n",
    "데이터 전처리 과정은 \n",
    "1. 산재하는 파일을 읽어서 사용하기 편한 형태로 변환하고,\n",
    "2. 불필요한 데이터를 정재하는 단계,\n",
    "3. 그리고 데이터 범위를 표준화하는 단계로 구성된다.\n",
    "\n",
    "iDASH2020 데이터셋의 기본 구조를 가정하여 Data loader가 작성되어있으며, 작동시에는 데이터 셋의 root 디렉토리 위치만 지정해주면 된다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 필요한 class를 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "\n",
    "from fase import seal\n",
    "from fase.RF.sktree import TumorRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TumorRF instance에 데이터의 root 디렉토리를 설정하여 데이터 전처리 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing done and train data are ready\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/home/hoseung/Dropbox/DeepInsight/2021ETRI/RF_Tumor_Classification/data/\"\n",
    "\n",
    "# Init TumorRf class\n",
    "# Pre-defined preprocessing steps are taken automatically. \n",
    "tumor_rf = TumorRF(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train - validation set \n",
    "\n",
    "모델을 훈련하고, 사용된 데이터셋을 pickle로 저장. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T13:49:16.741270Z",
     "start_time": "2020-06-06T13:49:16.719259Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Training set as tumor_train.pickle\n",
      "Saved Validation set as tumor_valid.pickle\n",
      "Training a Random Forest with 21 trees of depth 8.\n",
      "Train Done: train accuracy = 57.761%\n",
      "Test accuracy 57.737%\n"
     ]
    }
   ],
   "source": [
    "# Split the data and keep validation data separately. \n",
    "X_train, X_valid, Y_train, Y_valid = tumor_rf.split_data(fn_dump=\"tumor_\")\n",
    "\n",
    "# Train the model\n",
    "tumor_rf.train(X_train, Y_train, fn_out=\"trained_RF.pickle\", ntree=21, depth=8)\n",
    "\n",
    "# Test the model against validation dataset.\n",
    "y_preds = tumor_rf.predict(X_valid)\n",
    "\n",
    "# Caculate validation performance \n",
    "tumor_rf.accuracy(Y_valid, y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depth = 8일 때, 약 60%의 정확도를 얻었음. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fase.RF.sktree import extract_sk_dt\n",
    "RF = [extract_sk_dt(dt) for dt in tumor_rf.rf.estimators_]\n",
    "pickle.dump(RF, open(\"trained_rf_ver1\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"/home/hoseung/Work/data/trained_RF/\"\n",
    "\n",
    "trained_rf = pickle.load(open(model_dir+\"whole_rf.pickle\", 'rb'))\n",
    "dd = pickle.load(open(model_dir+\"validset.pickle\", 'rb'))\n",
    "\n",
    "X_valid = dd[\"valid_x\"]\n",
    "Y_valid = dd[\"valid_y\"]\n",
    "le = dd[\"label_encoder\"]\n",
    "\n",
    "assert (X_valid.min()>= -1) and (X_valid.max()) <= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T15:54:17.592037Z",
     "start_time": "2020-06-02T15:54:17.259957Z"
    }
   },
   "outputs": [],
   "source": [
    "from cryptotree.tree import NeuralRF#, SigmoidTreeMaker, TanhTreeMaker\n",
    "from cryptotree.polynomials import plot_graph_function_approximation\n",
    "\n",
    "max_depth = 8\n",
    "\n",
    "dilatation_factor = 10\n",
    "polynomial_degree = dilatation_factor\n",
    "\n",
    "#plot_graph_function_approximation(torch.tanh,\n",
    "#                                  dilatation_factor=dilatation_factor,\n",
    "#                                  polynomial_degree=polynomial_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cryptotree.tree import NeuralTreeMaker\n",
    "from sklearn.tree import BaseDecisionTree\n",
    "from functools import partial\n",
    "from cryptotree.tree import *\n",
    "from cryptotree.mine import *\n",
    "\n",
    "my_tm_tanh = NeuralTreeMaker(torch.tanh, \n",
    "                       use_polynomial=True,\n",
    "                                  dilatation_factor=dilatation_factor, polynomial_degree=polynomial_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T15:54:23.000386Z",
     "start_time": "2020-06-02T15:54:22.875406Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralRF()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tree_maker = tanh_tree_maker\n",
    "\n",
    "model =NeuralRF(trained_rf.rf.estimators_, tree_maker=my_tm_tanh)\n",
    "model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without finetuning\n",
    "\n",
    "각각의 estimator는 decision tree임. 여기서 \n",
    "tree.tree_.node_count\n",
    "tree.tree_.children_left\n",
    "tree.tree_.children_right\n",
    "tree.tree_.feature\n",
    "tree.tree_.threshold\n",
    "tree.n_features_ 추출해냄. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear layer. weight = one-hot code of feature at each node\n",
    "\n",
    "questions in all branching nodes \n",
    "\n",
    "Weight  \n",
    "[100000] - node 1 asks about feature 1  \n",
    "[010000] - node 2 asks about feature 2  \n",
    "[010000] - node 3 asks about feature 1  \n",
    "[000001] - node 4 asks about feature 6 ..  \n",
    "\n",
    "\n",
    "Bias   = threshold   \n",
    "[-0.4,  \n",
    " 0.4,  \n",
    " 0.6,  \n",
    " 0.9,  \n",
    " 0.2,  \n",
    " 0.1,  \n",
    " .  \n",
    " .  \n",
    " ...]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# matcher가 하는 일은..?   - 모든 branch node와 모든 leaf node를 연결\n",
    "\n",
    "예)  \n",
    "10번 leaf가  0 - 6 - 8 - 10 순으로 도착한다면 layer 1의 0,6,8번 정보를 합쳐서 layer2 -> prediction을 수행함.  \n",
    "\n",
    "\n",
    "NRF의 아이디어를 정리하자면,\n",
    "1) 모든 branch 노드를 weight과 bias로 표현  \n",
    "2) leaf마다 도달하는데 거치는 branch 노드들의 계산 결과의 조합으로 최종 결정을 내림.  \n",
    "그러니까, branch leaf에 도달하는 순서는 상관하지 않겠다는 의미. -- 왜냐면 yes면 1이고 no면 0이니까 순서랑 무관함.   \n",
    "\n",
    "그렇다면 layer를 더 쌓는다는게 의미가 있나? 말이 안 되는 건가..?  class 수가 많아지면 어떻게 하지..  \n",
    "\n",
    "\n",
    "한편,  \n",
    "NRF는 sklearn에서 train한 RF를 NN으로 옮기는 일. 그냥 처음부터 Linear 3개짜리 NN을 만들면 안 되나??  \n",
    "hyperparameter를 sklearn에서 결정해준다는 점은 의미가 있음.  \n",
    "\n",
    "crytotree에서는 서로 다른 크기/깊이의 tree를 가장 큰 tree에 맞춰 pad함.  (tree.py: L220)\n",
    "왜..? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid 트리는 성능이 매우 떨어짐. tanh는 얼추 비슷"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T15:54:21.256637Z",
     "start_time": "2020-06-02T15:54:18.994382Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_150923/585595069.py:7: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  print(f\"Original accuracy : {(pred[::100] == Y_valid[::100]).mean()}\")\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'bool' object has no attribute 'mean'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_150923/585595069.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrained_rf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Original accuracy : {(pred[::100] == Y_valid[::100]).mean()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#print(f\"Accuracy of sigmoid  : {(sigmoid_neural_pred == y_train).mean()}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'bool' object has no attribute 'mean'"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    #sigmoid_neural_pred = sigmoid_neural_rf(torch.tensor(X_train_normalized).float()).argmax(dim=1).numpy()\n",
    "    #tanh_neural_pred = tanh_neural_rf(torch.tensor(X_train_normalized).float()).argmax(dim=1).numpy()\n",
    "    tanh_neural_pred = model(torch.tensor(X_valid[::100]).float()).argmax(dim=1).numpy()\n",
    "    \n",
    "pred = trained_rf.predict(X_valid[::100])\n",
    "print(f\"Original accuracy : {(pred[::100] == Y_valid[::100]).mean()}\")\n",
    "\n",
    "#print(f\"Accuracy of sigmoid  : {(sigmoid_neural_pred == y_train).mean()}\")\n",
    "print(f\"Accuracy of tanh : {(tanh_neural_pred[::100] == Y_valid[::100]).mean()}\")\n",
    "\n",
    "#print(f\"Match between sigmoid and original : {(sigmoid_neural_pred == pred).mean()}\")\n",
    "print(f\"Match between tanh and original : {(tanh_neural_pred[::100] == pred[::100]).mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original accuracy : 0.5801656288368316\n",
    "Accuracy of tanh : 0.3580733051904536\n",
    "Match between tanh and original : 0.48896215385969977"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NRF도 FHE RF처럼 성능이 확 떨어짐...(1) tanh approx.가 문제인가 아니면 (2) class가 많은게 문제인가, 아니면 (3) Unbalanced class / 부족한 preprocessing이 문제인가? \n",
    "\n",
    "polynomial degree를 10에서 16으로 올려도 정확히 똑같은 결과. \n",
    "\n",
    "fine-tune하면 좋아짐."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With finetuning\n",
    "\n",
    "Here we first define our Pytorch Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T15:54:21.273755Z",
     "start_time": "2020-06-02T15:54:21.258166Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "import numpy as np\n",
    "\n",
    "class TabularDataset(data.Dataset):\n",
    "    \"\"\"minimum dataset class to convert a ndarray into a torch tensor\"\"\"\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
    "        self.X, self.y = X,y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load data and get label\n",
    "        X = torch.tensor(self.X[index]).float()\n",
    "        y = torch.tensor(self.y[index])\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we split our training data into training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = pickle.load(open(model_dir+\"trainset.pickle\", 'rb'))\n",
    "\n",
    "X_train = dd[\"train_x\"]\n",
    "Y_train = dd[\"train_y\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T15:54:21.322375Z",
     "start_time": "2020-06-02T15:54:21.274986Z"
    }
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=0.20, random_state=seed)\n",
    "X_train_normalized, X_valid_normalized, y_train, y_valid = train_test_split(pipe.transform(X_train), \n",
    "                                                                            y_train,\n",
    "                                                                            train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create the Pytorch dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T15:54:21.334525Z",
     "start_time": "2020-06-02T15:54:21.323394Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ds = TabularDataset(X_train, Y_train)\n",
    "valid_ds = TabularDataset(X_valid, Y_valid)\n",
    "\n",
    "bs = 128\n",
    "\n",
    "train_dl = data.DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "valid_dl = data.DataLoader(valid_ds, batch_size=bs)\n",
    "fix_dl = data.DataLoader(train_ds, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will just define the model, which is a sigmoid Neural Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we only want to train the last layer, we will freeze the first two layers and check they are frozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=8, max_features='auto',\n",
       "                       random_state=1150192196)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_rf.rf.estimators_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T15:54:23.243437Z",
     "start_time": "2020-06-02T15:54:23.227649Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 248, 21]) False\n",
      "torch.Size([248, 21]) False\n",
      "torch.Size([249, 248, 21]) False\n",
      "torch.Size([249, 21]) False\n",
      "torch.Size([11, 249, 21]) True\n",
      "torch.Size([11, 21]) True\n"
     ]
    }
   ],
   "source": [
    "model.freeze_layer(\"comparator\")\n",
    "model.freeze_layer(\"matcher\")\n",
    "\n",
    "for p in model.parameters():\n",
    "    print(p.shape, p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 메모리 부족 \n",
    "\n",
    "\n",
    "NeuralRF.forward()에서 모든 계산을 Einsum으로 하는데,  \n",
    "Einsum으로 계산하면 전체 input을 한번에 계산함. 16만 개를 한번에 하기는 좀 무리. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T15:54:24.709134Z",
     "start_time": "2020-06-02T15:54:23.743899Z"
    }
   },
   "outputs": [],
   "source": [
    "pred = trained_rf.predict(X_train)\n",
    "\n",
    "with torch.no_grad():\n",
    "    neural_pred = model(torch.tensor(X_train[:10000]).float()).argmax(dim=1).numpy()\n",
    "\n",
    "print(f\"Original accuracy : {(pred == Y_train).mean()}\")\n",
    "print(f\"Accuracy : {(neural_pred == Y_train[:10000]).mean()}\")\n",
    "print(f\"Same output : {(neural_pred == pred[:10000]).mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch fine tune\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import copy \n",
    "\n",
    "def train_model(model, \n",
    "                dataloader, \n",
    "                criterion, \n",
    "                optimizer, \n",
    "                scheduler, \n",
    "                num_epochs=25, \n",
    "                phase=\"train\"):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        # Iterate over data.\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            # track history if only in train\n",
    "            #with torch.set_grad_enabled(phase == 'train'):\n",
    "            #print(model.comparator.requires_grad)\n",
    "            #print(model.matcher.requires_grad)\n",
    "            #print(model.head.requires_grad)\n",
    "        \n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "            if phase == 'train':\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "        if phase == 'train':\n",
    "            scheduler.step()\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloader)\n",
    "        epoch_acc = running_corrects.double() / len(dataloader)\n",
    "\n",
    "        print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "            phase, epoch_loss, epoch_acc))\n",
    "        print(\"updating..?\", model.head[0].min(), model.head[0].max())\n",
    "        # deep copy the model\n",
    "        if phase == 'val' and epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    #model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating..? tensor(-0.1759, grad_fn=<MinBackward1>) tensor(0.4055, grad_fn=<MaxBackward1>)\n",
      "Epoch 0/1\n",
      "----------\n",
      "train Loss: 233.4254 Acc: 58.5607\n",
      "updating..? tensor(-0.3014, grad_fn=<MinBackward1>) tensor(0.6741, grad_fn=<MaxBackward1>)\n",
      "Epoch 1/1\n",
      "----------\n",
      "train Loss: 228.4141 Acc: 62.9072\n",
      "updating..? tensor(-0.4140, grad_fn=<MinBackward1>) tensor(0.9144, grad_fn=<MaxBackward1>)\n",
      "Training complete in 1m 26s\n",
      "Best val Acc: 0.000000\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "device = \"cpu\"\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = CrossEntropyLabelSmoothing()\n",
    "\n",
    "lr = 5e-3\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "model.train()\n",
    "\n",
    "print(\"updating..?\", model.head[0].min(), model.head[0].max())\n",
    "        \n",
    "model = train_model(model, train_dl, criterion, optimizer, exp_lr_scheduler,\n",
    "                       num_epochs=2, phase=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tune 되나? Fastai랑 다른거 없는 것 같은데... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating..? tensor(-0.4140, grad_fn=<MinBackward1>) tensor(0.9144, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(\"updating..?\", model.head[0].min(), model.head[0].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1\n",
      "----------\n",
      "train Loss: 237.3078 Acc: 55.5949\n",
      "Epoch 1/1\n",
      "----------\n",
      "train Loss: 237.3078 Acc: 55.5949\n",
      "Training complete in 1m 36s\n",
      "Best val Acc: 0.000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "since = time.time()\n",
    "\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "best_acc = 0.0\n",
    "num_epochs=2\n",
    "dataloader = train_dl\n",
    "model.train()\n",
    "phase=\"train\"\n",
    "scheduler=exp_lr_scheduler\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "    print('-' * 10)\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    # Iterate over data.\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        # track history if only in train\n",
    "        #with torch.set_grad_enabled(phase == 'train'):\n",
    "        outputs = model(inputs)\n",
    "        #print(model.comparator.requires_grad)\n",
    "        #print(model.matcher.requires_grad)\n",
    "        #print(model.head.requires_grad)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # backward + optimize only if in training phase\n",
    "        if phase == 'train':\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "    if phase == 'train':\n",
    "        scheduler.step()\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_acc = running_corrects.double() / len(dataloader)\n",
    "\n",
    "    print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "        phase, epoch_loss, epoch_acc))\n",
    "\n",
    "    # deep copy the model\n",
    "    if phase == 'val' and epoch_acc > best_acc:\n",
    "        best_acc = epoch_acc\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "    time_elapsed // 60, time_elapsed % 60))\n",
    "print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "# load best model weights\n",
    "model.load_state_dict(best_model_wts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "train Loss: 208.2449 Acc: 66.2010\n",
    "Epoch 23/24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define our fastai Learner, with the dataset, the model, and the loss function, which is a Label Smoothing Cross Entropy here.\n",
    "\n",
    "# Fastai 대체하기 \n",
    "\n",
    "너무 티나니까..\n",
    "\n",
    "Learner(data, model, loss_fuction, metrics)\n",
    "1. data:\n",
    "    pytorch dataloader과 똑같은 기능. Image와 label외에 bbox나 segmap등도 지원함.\n",
    "    \n",
    "2. model\n",
    "    torch 모델\n",
    "   \n",
    "3. loss_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import fastai\n",
    "fastai.__version__\n",
    "# Need Fastai 1, NOT 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T15:54:25.187874Z",
     "start_time": "2020-06-02T15:54:24.819611Z"
    }
   },
   "source": [
    "from fastai.basic_data import DataBunch\n",
    "from fastai.tabular.learner import Learner\n",
    "from fastai.metrics import accuracy\n",
    "\n",
    "from cryptotree.tree import CrossEntropyLabelSmoothing\n",
    "import torch.nn as nn\n",
    "\n",
    "data = DataBunch(train_dl, valid_dl,fix_dl=fix_dl)\n",
    "\n",
    "criterion = CrossEntropyLabelSmoothing()\n",
    "\n",
    "learn = Learner(data, model, loss_func=criterion, metrics=accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use fastai lr finder to have an idea of what learning rate to choose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T15:54:32.246884Z",
     "start_time": "2020-06-02T15:54:26.125394Z"
    }
   },
   "source": [
    "learn.lr_find(num_it=500)\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that a good learning rate should be around 1e-1.\n",
    "\n",
    "We can now fine tune our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T15:54:44.245642Z",
     "start_time": "2020-06-02T15:54:32.248618Z"
    }
   },
   "source": [
    "learn.fit_one_cycle(5,1e-2 / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can have a look at the performance of the Neural Random Forest tuned with respect to the original sklearn Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original accuracy : 0.5663046934233374\n",
      "Accuracy : 0.5084012033164576\n",
      "Same output : 0.8140362462396361\n"
     ]
    }
   ],
   "source": [
    "sl = slice(None, None, 5)\n",
    "\n",
    "pred = trained_rf.predict(X_valid[sl])\n",
    "\n",
    "with torch.no_grad():\n",
    "    neural_pred = model(torch.tensor(X_valid[sl]).float()).argmax(dim=1).numpy()\n",
    "\n",
    "print(f\"Original accuracy : {(pred == Y_valid[sl]).mean()}\")\n",
    "print(f\"Accuracy : {(neural_pred == Y_valid[sl]).mean()}\")\n",
    "print(f\"Same output : {(neural_pred == pred).mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fastai 결과  \n",
    "\n",
    "Original accuracy : 0.5663046934233374  \n",
    "Accuracy : 0.5171325849291951  \n",
    "Same output : 0.7578325629173087"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model, open(\"fine_tuned_torch.pickle\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fine tune으로 성능이 썩 좋아짐 :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homomorphic Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T15:54:50.619424Z",
     "start_time": "2020-06-02T15:54:50.601185Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 37]\n",
      "438\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "dilatation_factor = 16\n",
    "degree = dilatation_factor\n",
    "\n",
    "PRECISION_BITS = 28\n",
    "UPPER_BITS = 9\n",
    "\n",
    "polynomial_multiplications = int(np.ceil(np.log2(degree))) + 1\n",
    "n_polynomials = 2\n",
    "matrix_multiplications = 3\n",
    "\n",
    "depth = matrix_multiplications + polynomial_multiplications * n_polynomials\n",
    "\n",
    "poly_modulus_degree = 16384 # 2**14\n",
    "\n",
    "moduli = [PRECISION_BITS + UPPER_BITS] + (depth) * [PRECISION_BITS] + [PRECISION_BITS + UPPER_BITS]\n",
    "print(moduli)\n",
    "print(sum(moduli))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T15:54:44.435307Z",
     "start_time": "2020-06-02T15:54:44.247735Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original accuracy : 0.5427872860635696\n",
      "Accuracy : 0.3643031784841076\n",
      "Same output : 0.4841075794621027\n"
     ]
    }
   ],
   "source": [
    "sl = slice(None, None, 1000)\n",
    "\n",
    "\n",
    "pred = trained_rf.predict(X_valid[sl])\n",
    "\n",
    "with torch.no_grad():\n",
    "    neural_pred = model(torch.tensor(X_valid[sl]).float()).argmax(dim=1).numpy()\n",
    "\n",
    "print(f\"Original accuracy : {(pred == Y_valid[sl]).mean()}\")\n",
    "print(f\"Accuracy : {(neural_pred == Y_valid[sl]).mean()}\")\n",
    "print(f\"Same output : {(neural_pred == pred).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T15:55:03.131349Z",
     "start_time": "2020-06-02T15:54:52.461935Z"
    }
   },
   "outputs": [],
   "source": [
    "from cryptotree.seal_helper import create_seal_globals, append_globals_to_builtins\n",
    "import builtins\n",
    "\n",
    "create_seal_globals(globals(), poly_modulus_degree, moduli, PRECISION_BITS, use_symmetric_key=False)\n",
    "append_globals_to_builtins(globals(), builtins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then create our Homomorphic Random Forest from the Neural Random Forest we fine tuned earlier.\n",
    "\n",
    "We first have to extract the weights using the Homomorphic Random Forest class, and pass it to the Homomorphic Tree Evaluator which will do the computation, using the polynomial activation given, and the SEAL context.\n",
    "\n",
    "A featurizer is also created for the client side, in order to preprocess, encode and encrypt the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "b0, w1, b1, w2, b2 = h_rf.return_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10437"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nexample = len(b0)\n",
    "print(nexample, nexample / ntrees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight과 bias를 encode 해야하는데, 하나의 ciphertext에 다 안 들어감. \n",
    "# 1. Ciphertext를 키운다 -> 2**14\n",
    "# 2. 트리를 각각 계산한다. (지금은 21개를 하나로 합쳐서 10437개가 됨) -- 이게 더 맞는 말인 듯."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T15:55:03.341871Z",
     "start_time": "2020-06-02T15:55:03.133318Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_150923/2439849551.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                                      \u001b[0mtree_maker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoeffs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                                                      \u001b[0mpolyeval_tree\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                                                      \u001b[0mevaluator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                                                      \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                                                      \u001b[0mrelin_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluator' is not defined"
     ]
    }
   ],
   "source": [
    "from cryptotree.cryptotree import HomomorphicNeuralRandomForest, HomomorphicTreeEvaluator, HomomorphicTreeFeaturizer\n",
    "from cryptotree.polynomials import polyeval_tree\n",
    "\n",
    "h_rf = HomomorphicNeuralRandomForest(model)\n",
    "\n",
    "tree_maker = my_tm_tanh\n",
    "tree_evaluator = HomomorphicTreeEvaluator.from_model(h_rf, \n",
    "                                                     tree_maker.coeffs, \n",
    "                                                     polyeval_tree, \n",
    "                                                     evaluator, \n",
    "                                                     encoder, \n",
    "                                                     relin_keys, \n",
    "                                                     galois_keys, \n",
    "                                                     scale)\n",
    "\n",
    "homomorphic_featurizer = HomomorphicTreeFeaturizer(h_rf.return_comparator(), encoder, encryptor, scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take some data, encrypt it, and pass it to our Homorphic Tree Evaluator : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T15:55:07.065084Z",
     "start_time": "2020-06-02T15:55:03.343218Z"
    }
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "x = X_train_normalized[i]\n",
    "ctx = homomorphic_featurizer.encrypt(x)\n",
    "\n",
    "outputs = tree_evaluator(ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can decrypt and decode the data to see the output :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T08:36:23.734742Z",
     "start_time": "2020-06-01T08:36:23.716338Z"
    }
   },
   "outputs": [],
   "source": [
    "ptx = seal.Plaintext()\n",
    "decryptor.decrypt(outputs, ptx)\n",
    "\n",
    "homomorphic_pred = encoder.decode(ptx)[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see what was the original output, if we had used only the Neural Random Forest : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T08:36:25.347094Z",
     "start_time": "2020-06-01T08:36:25.314333Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Random Forest output : [[0.604789 0.395211]]\n",
      "Neural Random Forest output : tensor([[0.1420, 0.1236]])\n",
      "Homomorphic Random Forest output : [0.256069 0.002739]\n"
     ]
    }
   ],
   "source": [
    "x = X_train_normalized[i]\n",
    "\n",
    "pred = rf.predict_proba(x.reshape(1,-1))\n",
    "neural_pred = model(torch.tensor(x).float().unsqueeze(0))\n",
    "\n",
    "print(f\"Original Random Forest output : {pred}\")\n",
    "print(f\"Neural Random Forest output : {neural_pred.detach()}\")\n",
    "print(f\"Homomorphic Random Forest output : {homomorphic_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see our Neural Random Forest, and Homomorphic Random Forest have very similar outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison between models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now compare a linear model, a RF, a NRF and a HRF. \n",
    "\n",
    "Because computation are done one at a time with HRF, we will use Fastai `parallel` to speedup inference taking advantage of the multiple CPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T16:03:07.073814Z",
     "start_time": "2020-06-02T15:55:11.698789Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fastai.core import parallel\n",
    "import multiprocessing\n",
    "\n",
    "def predict(x, index):\n",
    "    \"\"\"Performs HRF prediction\"\"\"\n",
    "    \n",
    "    # We first encrypt and evaluate our model on it\n",
    "    ctx = homomorphic_featurizer.encrypt(x)\n",
    "    outputs = tree_evaluator(ctx)\n",
    "    \n",
    "    # We then decrypt it and get the first 2 values which are the classes scores\n",
    "    # ptx = seal.Plaintext()\n",
    "    ptx = decryptor.decrypt(outputs)\n",
    "    \n",
    "    homomorphic_pred = encoder.decode(ptx)[:2]\n",
    "    homomorphic_pred = np.argmax(homomorphic_pred)\n",
    "    \n",
    "    return index, homomorphic_pred\n",
    "\n",
    "# We get the number of cores\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "# We compute the outputs\n",
    "hrf_pred = parallel(predict, X_valid_normalized[:320,:], max_workers=cores)\n",
    "\n",
    "# Because the outputs are unordered we must first sort by index then take the predictions\n",
    "hrf_pred = np.array(sorted(hrf_pred, key = lambda x:x[0]))[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the predictions of the NRF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "\n",
    "for x,y in valid_dl:\n",
    "    with torch.no_grad():\n",
    "        pred = model(x)\n",
    "    outputs.append(pred)\n",
    "    \n",
    "nrf_pred = torch.cat(outputs).argmax(dim=1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we compute the predictions of logistic regression and RF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T15:20:09.451303Z",
     "start_time": "2020-06-05T15:20:09.130581Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "linear = LogisticRegression()\n",
    "linear.fit(X_train_normalized, y_train)\n",
    "\n",
    "# We compute the linear preds\n",
    "linear_pred = linear.predict(X_valid_normalized)\n",
    "\n",
    "# We compute the random forest predictions\n",
    "rf_pred = rf.predict(X_valid_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute all metrics now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T16:34:08.563287Z",
     "start_time": "2020-06-02T16:34:08.526218Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def compute_metrics(pred, y):\n",
    "    \"\"\"Computes all the metrics between predictions and real values\"\"\"\n",
    "    accuracy = accuracy_score(pred,y)\n",
    "    precision = precision_score(pred,y)\n",
    "    recall = recall_score(pred,y)\n",
    "    f1 = f1_score(pred, y)\n",
    "    return dict(accuracy=accuracy, precision=precision, recall=recall, f1=f1)\n",
    "\n",
    "models = dict(nrf=nrf_pred, hrf=hrf_pred, rf=rf_pred, linear=linear_pred)\n",
    "\n",
    "outputs = []\n",
    "for name, pred in models.items():\n",
    "    metrics = compute_metrics(pred[:320], y_valid[:320])\n",
    "    metrics[\"model\"] = name\n",
    "    outputs.append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T16:34:10.375109Z",
     "start_time": "2020-06-02T16:34:10.358017Z"
    }
   },
   "outputs": [],
   "source": [
    "outputs = pd.DataFrame(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T15:18:45.203482Z",
     "start_time": "2020-06-05T15:18:45.196446Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.809375</td>\n",
       "      <td>0.506494</td>\n",
       "      <td>0.629032</td>\n",
       "      <td>0.561151</td>\n",
       "      <td>nrf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.818750</td>\n",
       "      <td>0.467532</td>\n",
       "      <td>0.679245</td>\n",
       "      <td>0.553846</td>\n",
       "      <td>hrf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.821875</td>\n",
       "      <td>0.415584</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.528926</td>\n",
       "      <td>rf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.480519</td>\n",
       "      <td>0.649123</td>\n",
       "      <td>0.552239</td>\n",
       "      <td>linear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision    recall        f1   model\n",
       "0  0.809375   0.506494  0.629032  0.561151     nrf\n",
       "1  0.818750   0.467532  0.679245  0.553846     hrf\n",
       "2  0.821875   0.415584  0.727273  0.528926      rf\n",
       "3  0.812500   0.480519  0.649123  0.552239  linear"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T16:34:34.900805Z",
     "start_time": "2020-06-02T16:34:34.882706Z"
    }
   },
   "outputs": [],
   "source": [
    "outputs.to_csv(\"results.csv\", index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can have a look at how similar NRF and HRF predictions are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T16:34:36.991805Z",
     "start_time": "2020-06-02T16:34:36.974070Z"
    }
   },
   "outputs": [],
   "source": [
    "(nrf_pred == hrf_pred).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting featurizer and models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now see how we can export the model and the featurizer for the server and the client."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing pipeline can be saved for later use on the client side. We can save and load it with pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T17:26:27.937262Z",
     "start_time": "2020-05-15T17:26:27.899744Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(pipe, open(\"pipe.pkl\", \"wb\"))\n",
    "pipe = pickle.load(open(\"pipe.pkl\" , \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also store as well the featurizer, used to shuffle the features before the comparisons. This will be sent to the client side to prepare the data before sending it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T17:26:27.976904Z",
     "start_time": "2020-05-15T17:26:27.944267Z"
    }
   },
   "outputs": [],
   "source": [
    "homomorphic_featurizer.save(\"homomorphic_featurizer.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can store the Homomorphic Random Forest to use on the server side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T17:26:28.025277Z",
     "start_time": "2020-05-15T17:26:27.981667Z"
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump(h_rf, open(\"h_rf.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the demo built using Streamlit, we need to know what were the values of the categorical features, and also one example of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T17:26:28.114617Z",
     "start_time": "2020-05-15T17:26:28.028703Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-33-9b2a47c28999>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = X_train[col].astype(np.float32)\n",
      "<ipython-input-33-9b2a47c28999>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = X_train[col].astype(\"category\")\n"
     ]
    }
   ],
   "source": [
    "for col in X_train.columns.values:\n",
    "    if col in categorical_columns:\n",
    "        X_train[col] = X_train[col].astype(\"category\")\n",
    "    else:\n",
    "        X_train[col] = X_train[col].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T17:26:28.168300Z",
     "start_time": "2020-05-15T17:26:28.118573Z"
    }
   },
   "outputs": [],
   "source": [
    "example = X_train.iloc[[0]]\n",
    "example.to_csv(\"example.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T17:26:28.213964Z",
     "start_time": "2020-05-15T17:26:28.177644Z"
    }
   },
   "outputs": [],
   "source": [
    "dtypes = X_train.dtypes\n",
    "pickle.dump(dtypes, open(\"dtypes.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T17:26:28.260417Z",
     "start_time": "2020-05-15T17:26:28.217860Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age               float32\n",
       "WorkClass        category\n",
       "fnlwgt            float32\n",
       "Education        category\n",
       "EducationNum      float32\n",
       "MaritalStatus    category\n",
       "Occupation       category\n",
       "Relationship     category\n",
       "Race             category\n",
       "Gender           category\n",
       "CapitalGain       float32\n",
       "CapitalLoss       float32\n",
       "HoursPerWeek      float32\n",
       "NativeCountry    category\n",
       "dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtypes = pickle.load(open(\"dtypes.pkl\", \"rb\"))\n",
    "dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using exported models for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T17:34:49.629076Z",
     "start_time": "2020-05-15T17:34:48.003701Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from cryptotree.cryptotree import HomomorphicTreeFeaturizer, HomomorphicTreeEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T17:34:49.645508Z",
     "start_time": "2020-05-15T17:34:49.633528Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 37]\n",
      "438\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "dilatation_factor = 16\n",
    "degree = dilatation_factor\n",
    "\n",
    "PRECISION_BITS = 28\n",
    "UPPER_BITS = 9\n",
    "\n",
    "polynomial_multiplications = int(np.ceil(np.log2(degree))) + 1\n",
    "n_polynomials = 2\n",
    "matrix_multiplications = 3\n",
    "\n",
    "depth = matrix_multiplications + polynomial_multiplications * n_polynomials\n",
    "\n",
    "poly_modulus_degree = 16384\n",
    "\n",
    "moduli = [PRECISION_BITS + UPPER_BITS] + (depth) * [PRECISION_BITS] + [PRECISION_BITS + UPPER_BITS]\n",
    "print(moduli)\n",
    "print(sum(moduli))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T17:35:06.734187Z",
     "start_time": "2020-05-15T17:34:50.412813Z"
    }
   },
   "outputs": [],
   "source": [
    "from cryptotree.seal_helper import create_seal_globals, append_globals_to_builtins\n",
    "import builtins\n",
    "\n",
    "create_seal_globals(globals(), poly_modulus_degree, moduli, PRECISION_BITS, use_symmetric_key=False)\n",
    "append_globals_to_builtins(globals(), builtins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T17:35:06.765563Z",
     "start_time": "2020-05-15T17:35:06.737785Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe = pickle.load(open(\"pipe.pkl\",\"rb\"))\n",
    "homomorphic_featurizer = HomomorphicTreeFeaturizer.load(\"homomorphic_featurizer.pkl\",\n",
    "                                                        encoder, encryptor, scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T17:35:38.581279Z",
     "start_time": "2020-05-15T17:35:38.405390Z"
    }
   },
   "outputs": [],
   "source": [
    "from cryptotree.tree import SigmoidTreeMaker\n",
    "from cryptotree.polynomials import polyeval_tree\n",
    "\n",
    "dilatation_factor = 16\n",
    "polynomial_degree = dilatation_factor\n",
    "\n",
    "sigmoid_tree_maker = SigmoidTreeMaker(use_polynomial=True,\n",
    "                                  dilatation_factor=dilatation_factor, polynomial_degree=polynomial_degree)\n",
    "\n",
    "h_rf = pickle.load(open(\"h_rf.pkl\",\"rb\"))\n",
    "\n",
    "tree_evaluator = HomomorphicTreeEvaluator.from_model(h_rf, sigmoid_tree_maker.coeffs, \n",
    "                                                   polyeval_tree, evaluator, encoder, relin_keys, galois_keys, \n",
    "                                                   scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T17:35:57.029344Z",
     "start_time": "2020-05-15T17:35:56.628832Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "x = pd.read_csv(\"example.csv\")\n",
    "x = pipe.transform(x).reshape(-1)\n",
    "ctx = homomorphic_featurizer.encrypt(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T17:36:02.193751Z",
     "start_time": "2020-05-15T17:35:58.229683Z"
    }
   },
   "outputs": [],
   "source": [
    "outputs = tree_evaluator(ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T17:36:19.920269Z",
     "start_time": "2020-05-15T17:36:19.912895Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homomorphic Random Forest output : [0.7967431717299993, -0.9372202449111074]\n"
     ]
    }
   ],
   "source": [
    "ptx = seal.Plaintext()\n",
    "decryptor.decrypt(outputs, ptx)\n",
    "\n",
    "homomorphic_pred = encoder.decode_double(ptx)[:2]\n",
    "\n",
    "print(f\"Homomorphic Random Forest output : {homomorphic_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T17:33:54.681436Z",
     "start_time": "2020-05-15T17:33:54.637450Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Random Forest output : [[0.714852 0.285148]]\n",
      "Neural Random Forest output : tensor([[ 0.7526, -0.8541]])\n",
      "Homomorphic Random Forest output : [0.7528779109157453, -0.9063400792845879]\n"
     ]
    }
   ],
   "source": [
    "ptx = seal.Plaintext()\n",
    "decryptor.decrypt(outputs, ptx)\n",
    "\n",
    "homomorphic_pred = encoder.decode_double(ptx)[:2]\n",
    "\n",
    "pred = rf.predict_proba(x.reshape(1,-1))\n",
    "neural_pred = model(torch.tensor(x).float().unsqueeze(0))\n",
    "\n",
    "print(f\"Original Random Forest output : {pred}\")\n",
    "print(f\"Neural Random Forest output : {neural_pred.detach()}\")\n",
    "print(f\"Homomorphic Random Forest output : {homomorphic_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T16:03:13.395533Z",
     "start_time": "2020-06-02T16:03:13.378822Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "274.9px",
    "width": "214px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
