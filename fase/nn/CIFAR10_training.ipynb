{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4d12902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!  Training on GPU ...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d269571",
   "metadata": {},
   "source": [
    "Prepare Train / test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6db2eedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "num_workers = 0\n",
    "batch_size = 32\n",
    "valid_size = 0.2\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "train_data = datasets.CIFAR10('data', train=True,\n",
    "                              download=True, transform=transform)\n",
    "test_data = datasets.CIFAR10('data', train=False,\n",
    "                             download=True, transform=transform)\n",
    "\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# prepare data loaders (combine dataset and sampler)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "    sampler=train_sampler, num_workers=num_workers)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
    "    sampler=valid_sampler, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
    "    num_workers=num_workers)\n",
    "\n",
    "# specify the image classes\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2258d2d",
   "metadata": {},
   "source": [
    "## Simple CNN with two convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c006bf3f",
   "metadata": {},
   "source": [
    "7 (weighted) layer 이상을 쌓으려면... pool 한번에 절반씩 줄어들기 때문에 pool을 계속 하는 것도 이상함. pool 네 번이면 2x2로 작아짐. \n",
    "\n",
    "pooling 없이 conv만 하면? -- pooling이나 conv나 계산량이 비슷하므로 conv를 자꾸 하는 것도 나쁘지 않을 듯. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a327a8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (conv3): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "#activation = [F.relu, approx_relu][1]\n",
    "\n",
    "def approx_relu(x):\n",
    "    \"\"\"\n",
    "    Refer to Table 1 of\n",
    "    https://openreview.net/attachment?id=rkxsgkHKvH&name=original_pdf\n",
    "    \"\"\"\n",
    "    return 0.47 + 0.5*x + 0.09*x**2\n",
    "\n",
    "\n",
    "activation = approx_relu\n",
    "    \n",
    "class Net(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 8, 3)\n",
    "        self.conv2 = nn.Conv2d(8, 8, 3)\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        self.pool = nn.AvgPool2d(2, 2)\n",
    "        self.conv3 = nn.Conv2d(8, 16, 3)\n",
    "        self.conv4 = nn.Conv2d(16, 16, 3)\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        \n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = activation(self.bn1(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.conv3(x)\n",
    "        x = activation(self.bn2(self.conv4(x)))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = activation(self.fc1(x))\n",
    "        x = activation(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# create a complete CNN\n",
    "model = Net()\n",
    "model.to('cuda')\n",
    "train_on_gpu=True\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d8d38d",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2cad6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=.001, momentum=0.95)#, weight_decay=0.09)# WD makes \n",
    "#ptimizer = optim.Adam(model.parameters(), lr=.004, weight_decay=0.9)\n",
    "\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007e7939",
   "metadata": {},
   "source": [
    "FastAI training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d33c002",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.599134</td>\n",
       "      <td>1.581530</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.444366</td>\n",
       "      <td>1.420980</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.373034</td>\n",
       "      <td>1.370201</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.336980</td>\n",
       "      <td>1.316324</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.258206</td>\n",
       "      <td>1.269635</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.218838</td>\n",
       "      <td>1.226007</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.140185</td>\n",
       "      <td>1.188145</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.023007</td>\n",
       "      <td>1.252276</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.016322</td>\n",
       "      <td>1.318512</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.925849</td>\n",
       "      <td>1.179548</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.897124</td>\n",
       "      <td>1.158281</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.858345</td>\n",
       "      <td>1.198188</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.794668</td>\n",
       "      <td>1.179565</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.740147</td>\n",
       "      <td>1.359876</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.693829</td>\n",
       "      <td>1.723228</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.683222</td>\n",
       "      <td>1.540738</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.651285</td>\n",
       "      <td>1.334776</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.610485</td>\n",
       "      <td>1.341157</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.575251</td>\n",
       "      <td>1.424374</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.569229</td>\n",
       "      <td>1.333065</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.513944</td>\n",
       "      <td>1.532879</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fastai.vision.all import *\n",
    "import torchvision\n",
    "\n",
    "import fastai\n",
    "from fastai.optimizer import OptimWrapper\n",
    "\n",
    "\n",
    "from fastai.data.core import DataLoaders\n",
    "from fastai.learner import Learner\n",
    "from fastai.callback.progress import ProgressCallback\n",
    "#from fastai.callback.data import CudaCallback\n",
    "\n",
    "\n",
    "dls = DataLoaders(train_loader, test_loader)\n",
    "\n",
    "learn = Learner(dls, model, loss_func=criterion)#, opt_func=opt_func)#, cbs=[CudaCallback])\n",
    "\n",
    "learn.fine_tune(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585db7f0",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "de6acf62",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.012313 \tValidation Loss: 0.302897\n",
      "Validation loss decreased (inf --> 0.302897).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.991354 \tValidation Loss: 0.284357\n",
      "Validation loss decreased (0.302897 --> 0.284357).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.974319 \tValidation Loss: 0.278672\n",
      "Validation loss decreased (0.284357 --> 0.278672).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.963115 \tValidation Loss: 0.265368\n",
      "Validation loss decreased (0.278672 --> 0.265368).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.951391 \tValidation Loss: 0.267628\n",
      "Epoch: 6 \tTraining Loss: 0.941690 \tValidation Loss: 0.273839\n",
      "Epoch: 7 \tTraining Loss: 0.935993 \tValidation Loss: 0.250015\n",
      "Validation loss decreased (0.265368 --> 0.250015).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 0.927553 \tValidation Loss: 0.270340\n",
      "Epoch: 9 \tTraining Loss: 0.918959 \tValidation Loss: 0.253501\n",
      "Epoch: 10 \tTraining Loss: 0.910461 \tValidation Loss: 0.249090\n",
      "Validation loss decreased (0.250015 --> 0.249090).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 0.906314 \tValidation Loss: 0.269346\n",
      "Epoch: 12 \tTraining Loss: 0.899665 \tValidation Loss: 0.244859\n",
      "Validation loss decreased (0.249090 --> 0.244859).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 0.892598 \tValidation Loss: 0.239871\n",
      "Validation loss decreased (0.244859 --> 0.239871).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 0.887648 \tValidation Loss: 0.253405\n",
      "Epoch: 15 \tTraining Loss: 0.881874 \tValidation Loss: 0.244878\n",
      "Epoch: 16 \tTraining Loss: 0.874028 \tValidation Loss: 0.261430\n",
      "Epoch: 17 \tTraining Loss: 0.870294 \tValidation Loss: 0.235839\n",
      "Validation loss decreased (0.239871 --> 0.235839).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 0.866189 \tValidation Loss: 0.244513\n",
      "Epoch: 19 \tTraining Loss: 0.860130 \tValidation Loss: 0.263703\n",
      "Epoch: 20 \tTraining Loss: 0.855824 \tValidation Loss: 0.260663\n",
      "Epoch: 21 \tTraining Loss: 0.851099 \tValidation Loss: 0.265631\n",
      "Epoch: 22 \tTraining Loss: 0.848277 \tValidation Loss: 0.265614\n",
      "Epoch: 23 \tTraining Loss: 0.841758 \tValidation Loss: 0.238415\n",
      "Epoch: 24 \tTraining Loss: 0.838493 \tValidation Loss: 0.238597\n",
      "Epoch: 25 \tTraining Loss: 0.835285 \tValidation Loss: 0.277285\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 2\n",
    "train_losslist=[]\n",
    "valid_loss_min = np.Inf \n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    for data, target in train_loader:\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    model.eval()\n",
    "    for data, target in valid_loader:\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(valid_loader.dataset)\n",
    "    train_losslist.append(train_loss)\n",
    "        \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss))\n",
    "    \n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        #torch.save(model.state_dict(), 'model_cifar_2fc.pt')\n",
    "        valid_loss_min = valid_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d59b894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290caa09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5743f22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"Net_3fc2act.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc8936e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fb9607",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8901955e",
   "metadata": {},
   "source": [
    "## Validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f62ee965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.059538\n",
      "\n",
      "Test Accuracy of airplane: 71% (719/1000)\n",
      "Test Accuracy of automobile: 79% (794/1000)\n",
      "Test Accuracy of  bird: 53% (533/1000)\n",
      "Test Accuracy of   cat: 48% (487/1000)\n",
      "Test Accuracy of  deer: 64% (649/1000)\n",
      "Test Accuracy of   dog: 54% (541/1000)\n",
      "Test Accuracy of  frog: 77% (775/1000)\n",
      "Test Accuracy of horse: 70% (704/1000)\n",
      "Test Accuracy of  ship: 78% (782/1000)\n",
      "Test Accuracy of truck: 74% (747/1000)\n",
      "\n",
      "Test Accuracy (Overall): 67% (6731/10000)\n"
     ]
    }
   ],
   "source": [
    "# track test loss\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=32, \n",
    "    num_workers=num_workers)\n",
    "\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "model.eval()\n",
    "# iterate over test data\n",
    "for data, target in test_loader:\n",
    "    if train_on_gpu:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "    output = model(data)\n",
    "    loss = criterion(output, target)\n",
    "    test_loss += loss.item()*data.size(0)\n",
    "    _, pred = torch.max(output, 1)    \n",
    "    correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "# average test loss\n",
    "test_loss = test_loss/len(test_loader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            classes[i], 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ecb343",
   "metadata": {},
   "source": [
    "## 성능 \n",
    "\n",
    "(Conv - BN - Avg Pool)\n",
    "conv 5x5, same padding = 67\n",
    "conv 3x3, same padding = 68\n",
    "conv 3x3, valid padding = 67\n",
    "\n",
    "\n",
    "\n",
    "--------------------------\n",
    "\n",
    "relu + maxpool: ~62%\n",
    "\n",
    "relu + avgpool: ~58% -- OK, maxpool -> avgpool은 큰 문제 없음. \n",
    "\n",
    "approx. relu + avgpool: 52% !! \n",
    "\n",
    "approx. relu + avgpool + BN (2Conv + 3FC, 20 epoch): 58% 정도? \n",
    "\n",
    "approx. relu + avgpool + BN (2Conv + 2FC, 50 epoch: 59% \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dcb399",
   "metadata": {},
   "source": [
    "## Number of trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4d2c87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have to train 62006 parameters\n"
     ]
    }
   ],
   "source": [
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(f\"We have to train {params} parameters\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
