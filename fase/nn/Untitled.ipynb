{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fd0df73",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1459e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from fase.nn.conv import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78ef23a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "num_workers = 0\n",
    "batch_size = 32\n",
    "valid_size = 0.2\n",
    "\n",
    "\n",
    "## Scale \n",
    "transform = transforms.Compose([\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "     ])\n",
    "\n",
    "train_data = datasets.CIFAR10('data', train=True,\n",
    "                              download=True,\n",
    "                              transform=transform\n",
    "                             )\n",
    "test_data = datasets.CIFAR10('data', train=False,\n",
    "                             download=True, \n",
    "                             transform=transform\n",
    "                            )\n",
    "\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# prepare data loaders (combine dataset and sampler)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "    sampler=train_sampler, num_workers=num_workers)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
    "    sampler=valid_sampler, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
    "    num_workers=num_workers)\n",
    "\n",
    "# specify the image classes\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c907d486",
   "metadata": {},
   "outputs": [],
   "source": [
    "from approximate import approx_relu, approx_sign\n",
    "from fase.nn.models import ConvNeuralNet\n",
    "\n",
    "xfactor = 40\n",
    "activation = lambda x : xfactor * approx_relu(x/xfactor, degree = 5, repeat=3)\n",
    "\n",
    "org_model = ConvNeuralNet(num_classes=10, activation=activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c79a824",
   "metadata": {},
   "source": [
    "# FHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378da211",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import fase\n",
    "from fase.core import seal_ckks\n",
    "from fase.core.seal_ckks import SEALContext\n",
    "from fase.seal import Ciphertext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2801397f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEAL CKKS scheme is ready\n"
     ]
    }
   ],
   "source": [
    "poly_modulus_degree = 2**15\n",
    "scale_bit = 50\n",
    "coeff_moduli = [60] + [scale_bit] * 14 + [60]\n",
    "\n",
    "sec = SEALContext(poly_modulus_degree=poly_modulus_degree,\n",
    "                             coeff_moduli=coeff_moduli,\n",
    "                             scale_bit=scale_bit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837f80eb",
   "metadata": {},
   "source": [
    "## Load image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3738c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "img = np.array(Image.open(\"./bird6.png\"))\n",
    "to_tensor = transforms.ToTensor() # [n_channel, nh, nw]\n",
    "img_tensor = to_tensor(img).unsqueeze(0) # [n_batch, n_channel, nh, nw]\n",
    "n_batch, n_channel, nh, nw = img_tensor.shape\n",
    "\n",
    "print(img_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a127eea3",
   "metadata": {},
   "source": [
    "## Load trained parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "583d04d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_param = \"SimpleCNN_ReLU_minimax_v2.pt\"\n",
    "trained_param = torch.load(fn_param)\n",
    "trained_param = {k: v.cpu() for k, v in trained_param.items()} # to cpu()\n",
    "org_model.load_state_dict(trained_param)\n",
    "org_model.eval() ## If not eval(), running_mean and running_var of batch_norm changes\n",
    "\n",
    "# To numpy\n",
    "params_np = {k: v.numpy() for k, v in trained_param.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4c55189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fase.nn.utils as utils\n",
    "util = utils.Seal_checker(sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b11d3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0824) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# 테스트용 이미지 (새_)\n",
    "\n",
    "img_this_example = img_tensor[0] # Assume batch size = 1\n",
    "img_enc = [sec.encrypt(this_channel.ravel()) for this_channel in img_this_example]\n",
    "\n",
    "\n",
    "#util.check_decrypt(img_enc[0])\n",
    "\n",
    "print(img_tensor.min(), img_tensor.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e1d2f4",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f7b6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1, _nh2, _nw2 = my_conv2D_FHE(sec, img_enc, nh, nw, org_model.conv_layer1.weight) # list of ctxts\n",
    "tmp2, _nh2, _nw2 = my_conv2D_FHE(sec, tmp1, nh, nw, org_model.conv_layer2.weight) # list of ctxts\n",
    "tmp3, _nh2, _nw2 = fhe_avg_pool(sec, tmp2, nh, nw, \n",
    "                                kernel_size=org_model.pool.kernel_size, \n",
    "                                stride_in=1)\n",
    "tmp4, _nh2, _nw2 = my_conv2D_FHE(sec, tmp3, nh, nw, org_model.conv_layer3.weight,\n",
    "                                stride_in=2) \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
